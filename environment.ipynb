{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Now we have a trainable discriminator - it's time to build the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:51:34.969516Z",
     "start_time": "2020-03-11T21:51:34.900935Z"
    }
   },
   "outputs": [],
   "source": [
    "## Imports and data loading\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "import gym\n",
    "\n",
    "from musicrl.midi2vec import MidiVectorMapper, PostProcessor\n",
    "from musicrl.render import *\n",
    "from musicrl.random_generator import resemble_midi, repair_generated_seq\n",
    "from musicrl.data import RandomMidiDataGenerator\n",
    "from musicrl import mel_lstm\n",
    "\n",
    "import pretty_midi\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "REAL = 1\n",
    "GEN = 0\n",
    "\n",
    "na = None # new axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.443907Z",
     "start_time": "2020-03-11T21:51:39.483788Z"
    }
   },
   "outputs": [],
   "source": [
    "filepaths = list(glob('maestro-v2.0.0/2008/**.midi'))\n",
    "real_midis = [pretty_midi.PrettyMIDI(i) for i in filepaths]\n",
    "mapper = MidiVectorMapper(real_midis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.537375Z",
     "start_time": "2020-03-11T21:52:09.446036Z"
    }
   },
   "outputs": [],
   "source": [
    "mapper = MidiVectorMapper(real_midis)\n",
    "real_seq = mapper.midi2vec(real_midis[1])\n",
    "real_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.592023Z",
     "start_time": "2020-03-11T21:52:09.539474Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapper.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.675168Z",
     "start_time": "2020-03-11T21:52:09.595179Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "import pretty_midi\n",
    "\n",
    "\n",
    "class MelEnvironment(gym.Env):\n",
    "    \"\"\"Environment to train generating midi data in a self defined\n",
    "    vector space. The midi vector representation is defined via the\n",
    "    mapper object.\n",
    "    The waveform for the single instrument is then preprocessed for the\n",
    "    discriminator, and at each time step, the discriminators final prediction\n",
    "    serves as reward.\n",
    "    The preprocessed waveform, i.e. the mel spectrogram, also serves as\n",
    "    observation. The number of time frames that are used for the observation\n",
    "    are defined by the constant `self.N_TIMESTEPS`.\n",
    "    One session is understood as one song.\n",
    "    \n",
    "    Always works on batches - i.e. multiple songs/trajectories in parallel\n",
    "    \n",
    "    Gets:\n",
    "        discriminator: keras.Model: np.array(preprocessed) -> np.array(#time_steps, 1)\n",
    "        preprocess: function: np.array(#actions): waveform -> np.array(preprocessed) : spectrogram\n",
    "        mapper: musicrl.midi2vec.MidiVectorMapper\n",
    "        N_TIMESTEPS: int: number of timesteps used to generate the observation\n",
    "        MAX_NUM_ACTIONS: int: number of actions after which to end a trajectory\n",
    "    \"\"\"\n",
    "    def __init__(self, discriminator, preprocess_wav, mapper, N_TIMESTEPS=100, MAX_NUM_ACTIONS=10000):\n",
    "        super().__init__()\n",
    "        # N_TIMESTEPS is used to define the observation:\n",
    "        # This many timeframes of the spectrogram are fed\n",
    "        # back to the generator\n",
    "        self.N_TIMESTEPS = N_TIMESTEPS\n",
    "        self.MAX_NUM_ACTIONS = MAX_NUM_ACTIONS\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = gym.spaces.Box(0, np.inf, shape=(mapper.dims,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                           shape=(self.N_TIMESTEPS, 128), dtype=np.float32)\n",
    "        self.discriminator = discriminator\n",
    "        self.preprocess_wav = preprocess_wav\n",
    "        self.mapper = mapper\n",
    "        self.fr = 44100\n",
    "        self.rewards = []\n",
    "        self.current_seq = []\n",
    "        self.current_midi = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=300)\n",
    "        self.current_midi.instruments.append(pretty_midi.Instrument(program=0))\n",
    "        self.current_observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "     \n",
    "    def _update_wav(self, action):\n",
    "        \"\"\"Appends the action to the current_seq and synthesizes the sound\n",
    "        if an actual note was played\n",
    "        \n",
    "        Gets:\n",
    "            action: np.array of shape (mapper.dims)\n",
    "        Returns:\n",
    "            updated: Boolean, True iff a note was played\n",
    "        \"\"\"\n",
    "        self.current_seq.append(action)\n",
    "        event = mapper.action2note(action, start=len(self.current_seq)*mapper.time_per_tick)\n",
    "        if isinstance(event, pretty_midi.Note):\n",
    "            if len(self.current_midi.instruments[0].notes) == 1:\n",
    "                # It is the first note, so we synthesize\n",
    "                self.current_midi.instruments[0].notes.append(event)\n",
    "                self.current_midi.instruments[0].synthesize(self.fr)\n",
    "            else:\n",
    "                self.current_midi.instruments[0].append_and_synthesize(event)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def step(self, action):\n",
    "        if not self._update_wav(action):\n",
    "            return self.current_observation, 0, action[4]>0.5, None\n",
    "        preprocessed = self.preprocess_wav(self.current_wav, self.fr)[na]\n",
    "        prediction = self.discriminator.predict_on_batch(preprocessed)\n",
    "        observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "        observation[-min(self.N_TIMESTEPS, len(preprocessed[0])):] = preprocessed[0, -self.N_TIMESTEPS:]\n",
    "        self.current_observation = observation\n",
    "        self.current_prediction = prediction\n",
    "        reward = prediction[0, -1, 0]\n",
    "        self.rewards.append(reward)\n",
    "        # TODO: add a end token to mapper (issue #1)\n",
    "        done = len(self.current_seq) >= self.MAX_NUM_ACTIONS\n",
    "        return observation, reward, done, None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_seq = []\n",
    "        self.current_midi = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=300)\n",
    "        self.current_midi.instruments.append(pretty_midi.Instrument(program=0))\n",
    "        self.current_observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "        self.rewards = []\n",
    "        return self.current_observation\n",
    "    \n",
    "    @property\n",
    "    def current_wav(self):\n",
    "        return self.current_midi.instruments[0].synthesized\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        plot_spectro(self.current_observation.T, \"Current observation\")\n",
    "    \n",
    "    def close (self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the environment by pretending to take actions of a real midi sequence, just to check that everything works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:02:31.717307Z",
     "start_time": "2020-03-11T21:58:35.545209Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = load_model(\"models/mel_lstm.h5\")\n",
    "\n",
    "env = MelEnvironment(discriminator, mel_lstm.preprocess_wav, mapper, 1000)\n",
    "\n",
    "observations = []\n",
    "for i, action in enumerate(real_seq):\n",
    "    observation, _, _, _ = env.step(action)\n",
    "    observations.append(observation)\n",
    "    if i+1 % 10000 == 0:\n",
    "        env.render()\n",
    "        plt.show()\n",
    "        break\n",
    "        \n",
    "    \n",
    "# display(Audio(env.current_wav, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:02:38.670064Z",
     "start_time": "2020-03-11T22:02:38.312671Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:21:49.363472Z",
     "start_time": "2020-03-11T20:21:48.221802Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(env.current_wav, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:50:17.361799Z",
     "start_time": "2020-03-11T20:50:14.981230Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "again = mapper.vec2midi(env.current_seq)\n",
    "listen_to(again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:52:33.111298Z",
     "start_time": "2020-03-11T20:52:30.653824Z"
    }
   },
   "outputs": [],
   "source": [
    "listen_to(real_midis[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:55:43.285501Z",
     "start_time": "2020-03-11T20:55:42.942902Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(env.rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T23:27:50.328498Z",
     "start_time": "2020-03-11T23:27:49.167371Z"
    }
   },
   "outputs": [],
   "source": [
    "real_seq = mapper.midi2vec(real_midis[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "In order to implement the actor, we need a generator that takes observations and outputs a state. Before we build stuff for the reinforcement learning training loop, we will try to find an architecture that takes realistic inputs and generates something playable.\n",
    "\n",
    "I have implemented the `midi2vec.postprocess` such that it transforms the output of an untrained LSTM into something that is not just silence - otherwise, the actor will never play a keyboard, never get any reward and never learn anything. A drama that we need to avoid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T23:40:47.876988Z",
     "start_time": "2020-03-11T23:39:28.257236Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128,\n",
    "        return_sequences=False,\n",
    "        batch_input_shape=(10000, 1, 128000),\n",
    "        stateful=True))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(mapper.dims, activation='relu'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[metrics.binary_accuracy])\n",
    "\n",
    "postprocess = PostProcessor([mapper.midi2vec(real_midi) for real_midi in real_midis[:5]])\n",
    "\n",
    "observations = np.array(observations[:10000])\n",
    "states = observations.reshape(10000, 1, -1)\n",
    "gen_seq = model.predict(states)\n",
    "gen_seq = postprocess(gen_seq)\n",
    "gen_midi = mapper.vec2midi(gen_seq)\n",
    "listen_to(gen_midi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random generator on new `MidiVectorMapper`\n",
    "\n",
    "This is a small leftover from developing the postprocessor - and a demonstration that it is now not so hard anymore to generate something. Gaussian noise almost does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T23:40:59.642282Z",
     "start_time": "2020-03-11T23:40:47.889692Z"
    }
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0.2, 1, size=(15000, 5))\n",
    "noise[:,4] = 0\n",
    "rand_seq = postprocess(noise)\n",
    "rand_midi = mapper.vec2midi(rand_seq)\n",
    "listen_to(rand_midi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('midi-rl': conda)",
   "language": "python",
   "name": "python37664bitmidirlcondaf7dd53044da64cd186e20414f04e4f3e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
