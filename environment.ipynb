{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Now we have a trainable discriminator - it's time to build the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:51:34.969516Z",
     "start_time": "2020-03-11T21:51:34.900935Z"
    }
   },
   "outputs": [],
   "source": [
    "## Imports and data loading\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "import gym\n",
    "\n",
    "from musicrl.midi2vec import MidiVectorMapper, PostProcessor\n",
    "from musicrl.render import *\n",
    "from musicrl.random_generator import resemble_midi, repair_generated_seq\n",
    "from musicrl.data import RandomMidiDataGenerator\n",
    "from musicrl import mel_lstm\n",
    "\n",
    "import pretty_midi\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "REAL = 1\n",
    "GEN = 0\n",
    "\n",
    "na = None # new axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.443907Z",
     "start_time": "2020-03-11T21:51:39.483788Z"
    }
   },
   "outputs": [],
   "source": [
    "filepaths = list(glob('maestro-v2.0.0/2008/**.midi'))\n",
    "real_midis = [pretty_midi.PrettyMIDI(i) for i in filepaths]\n",
    "mapper = MidiVectorMapper(real_midis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.537375Z",
     "start_time": "2020-03-11T21:52:09.446036Z"
    }
   },
   "outputs": [],
   "source": [
    "mapper = MidiVectorMapper(real_midis)\n",
    "real_seq = mapper.midi2vec(real_midis[1])\n",
    "real_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.592023Z",
     "start_time": "2020-03-11T21:52:09.539474Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapper.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T21:52:09.675168Z",
     "start_time": "2020-03-11T21:52:09.595179Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "import pretty_midi\n",
    "\n",
    "\n",
    "class MelEnvironment(gym.Env):\n",
    "    \"\"\"Environment to train generating midi data in a self defined\n",
    "    vector space. The midi vector representation is defined via the\n",
    "    mapper object.\n",
    "    The waveform for the single instrument is then preprocessed for the\n",
    "    discriminator, and at each time step, the discriminators final prediction\n",
    "    serves as reward.\n",
    "    The preprocessed waveform, i.e. the mel spectrogram, also serves as\n",
    "    observation. The number of time frames that are used for the observation\n",
    "    are defined by the constant `self.N_TIMESTEPS`.\n",
    "    One session is understood as one song.\n",
    "    \n",
    "    Always works on batches - i.e. multiple songs/trajectories in parallel\n",
    "    \n",
    "    Gets:\n",
    "        discriminator: keras.Model: np.array(preprocessed) -> np.array(#time_steps, 1)\n",
    "        preprocess: function: np.array(#actions): waveform -> np.array(preprocessed) : spectrogram\n",
    "        mapper: musicrl.midi2vec.MidiVectorMapper\n",
    "        N_TIMESTEPS: int: number of timesteps used to generate the observation\n",
    "        MAX_NUM_ACTIONS: int: number of actions after which to end a trajectory\n",
    "    \"\"\"\n",
    "    def __init__(self, discriminator, preprocess_wav, mapper, N_TIMESTEPS=100, MAX_NUM_ACTIONS=10000):\n",
    "        super().__init__()\n",
    "        # N_TIMESTEPS is used to define the observation:\n",
    "        # This many timeframes of the spectrogram are fed\n",
    "        # back to the generator\n",
    "        self.N_TIMESTEPS = N_TIMESTEPS\n",
    "        self.MAX_NUM_ACTIONS = MAX_NUM_ACTIONS\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = gym.spaces.Box(0, np.inf, shape=(mapper.dims,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                           shape=(self.N_TIMESTEPS, 128), dtype=np.float32)\n",
    "        self.discriminator = discriminator\n",
    "        self.preprocess_wav = preprocess_wav\n",
    "        self.mapper = mapper\n",
    "        self.fr = 44100\n",
    "        self.rewards = []\n",
    "        self.current_seq = []\n",
    "        self.current_midi = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=300)\n",
    "        self.current_midi.instruments.append(pretty_midi.Instrument(program=0))\n",
    "        self.current_observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "     \n",
    "    def _update_wav(self, action):\n",
    "        \"\"\"Appends the action to the current_seq and synthesizes the sound\n",
    "        if an actual note was played\n",
    "        \n",
    "        Gets:\n",
    "            action: np.array of shape (mapper.dims)\n",
    "        Returns:\n",
    "            updated: Boolean, True iff a note was played\n",
    "        \"\"\"\n",
    "        self.current_seq.append(action)\n",
    "        event = mapper.action2note(action, start=len(self.current_seq)*mapper.time_per_tick)\n",
    "        if isinstance(event, pretty_midi.Note):\n",
    "            if len(self.current_midi.instruments[0].notes) == 1:\n",
    "                # It is the first note, so we synthesize\n",
    "                self.current_midi.instruments[0].notes.append(event)\n",
    "                self.current_midi.instruments[0].synthesize(self.fr)\n",
    "            else:\n",
    "                self.current_midi.instruments[0].append_and_synthesize(event)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def step(self, action):\n",
    "        if not self._update_wav(action):\n",
    "            return self.current_observation, 0, action[4]>0.5, None\n",
    "        preprocessed = self.preprocess_wav(self.current_wav, self.fr)[na]\n",
    "        prediction = self.discriminator.predict_on_batch(preprocessed)\n",
    "        observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "        observation[-min(self.N_TIMESTEPS, len(preprocessed[0])):] = preprocessed[0, -self.N_TIMESTEPS:]\n",
    "        self.current_observation = observation\n",
    "        self.current_prediction = prediction\n",
    "        reward = prediction[0, -1, 0]\n",
    "        self.rewards.append(reward)\n",
    "        # TODO: add a end token to mapper (issue #1)\n",
    "        done = len(self.current_seq) >= self.MAX_NUM_ACTIONS\n",
    "        return observation, reward, done, None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_seq = []\n",
    "        self.current_midi = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=300)\n",
    "        self.current_midi.instruments.append(pretty_midi.Instrument(program=0))\n",
    "        self.current_observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "        self.rewards = []\n",
    "        return self.current_observation\n",
    "    \n",
    "    @property\n",
    "    def current_wav(self):\n",
    "        return self.current_midi.instruments[0].synthesized\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        plot_spectro(self.current_observation.T, \"Current observation\")\n",
    "    \n",
    "    def close (self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the environment by pretending to take actions of a real midi sequence, just to check that everything works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:02:31.717307Z",
     "start_time": "2020-03-11T21:58:35.545209Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = load_model(\"models/mel_lstm.h5\")\n",
    "\n",
    "env = MelEnvironment(discriminator, mel_lstm.preprocess_wav, mapper, 1000)\n",
    "\n",
    "observations = []\n",
    "for i, action in enumerate(real_seq):\n",
    "    observation, _, _, _ = env.step(action)\n",
    "    observations.append(observation)\n",
    "    if i+1 % 10000 == 0:\n",
    "        env.render()\n",
    "        plt.show()\n",
    "        break\n",
    "        \n",
    "    \n",
    "# display(Audio(env.current_wav, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:02:38.670064Z",
     "start_time": "2020-03-11T22:02:38.312671Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:21:49.363472Z",
     "start_time": "2020-03-11T20:21:48.221802Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(env.current_wav, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:50:17.361799Z",
     "start_time": "2020-03-11T20:50:14.981230Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "again = mapper.vec2midi(env.current_seq)\n",
    "listen_to(again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:52:33.111298Z",
     "start_time": "2020-03-11T20:52:30.653824Z"
    }
   },
   "outputs": [],
   "source": [
    "listen_to(real_midis[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T20:55:43.285501Z",
     "start_time": "2020-03-11T20:55:42.942902Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(env.rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T23:27:50.328498Z",
     "start_time": "2020-03-11T23:27:49.167371Z"
    }
   },
   "outputs": [],
   "source": [
    "real_seq = mapper.midi2vec(real_midis[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "In order to implement the actor, we need a generator that takes observations and outputs a state. Before we build stuff for the reinforcement learning training loop, we will try to find an architecture that takes realistic inputs and generates something playable.\n",
    "\n",
    "I have implemented the `midi2vec.postprocess` such that it transforms the output of an untrained LSTM into something that is not just silence - otherwise, the actor will never play a keyboard, never get any reward and never learn anything. A drama that we need to avoid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(batch_input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(128,\n",
    "            return_sequences=False,\n",
    "            batch_input_shape=batch_input_shape,\n",
    "            stateful=True))\n",
    "    model.add(Dense(128, activation='sigmoid'))\n",
    "    model.add(Dense(mapper.dims, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[metrics.binary_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T23:40:47.876988Z",
     "start_time": "2020-03-11T23:39:28.257236Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_generator(batch_input_shape=(10000, 1, 128000))\n",
    "\n",
    "postprocess = PostProcessor([mapper.midi2vec(real_midi) for real_midi in real_midis[:5]])\n",
    "\n",
    "observations = np.array(observations[:10000])\n",
    "states = observations.reshape(10000, 1, -1)\n",
    "gen_seq = model.predict(states)\n",
    "gen_seq = postprocess(gen_seq)\n",
    "gen_midi = mapper.vec2midi(gen_seq)\n",
    "listen_to(gen_midi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the generator in the environment, without training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "for i in range(15000):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the environment for off policy batch training\n",
    "\n",
    "Generating a single trajectory takes a lot of time, even without training. This is due to the fact that we generate step by step, and after every step we generate the spectrogram from scatch.\n",
    "\n",
    "While the environment itself is optimizable - i.e. by only computing the spectrograms for frames in which new sound has been added, the most obvious speed up can be achieved by batch training.\n",
    "\n",
    "Batch training means, we let the same agent generate `batch_size` songs/trajectories in parallel. The discriminator also predicts for so many songs in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random generator on new `MidiVectorMapper`\n",
    "\n",
    "This is a small leftover from developing the postprocessor - and a demonstration that it is now not so hard anymore to generate something. Gaussian noise almost does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T23:40:59.642282Z",
     "start_time": "2020-03-11T23:40:47.889692Z"
    }
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0.2, 1, size=(15000, 5))\n",
    "noise[:,4] = 0\n",
    "rand_seq = postprocess(noise)\n",
    "rand_midi = mapper.vec2midi(rand_seq)\n",
    "listen_to(rand_midi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('midi-rl': conda)",
   "language": "python",
   "name": "python37664bitmidirlcondaf7dd53044da64cd186e20414f04e4f3e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
