{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "Now we have a trainable discriminator - it's time to build the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T12:35:33.158718Z",
     "start_time": "2020-03-11T12:35:33.090468Z"
    }
   },
   "outputs": [],
   "source": [
    "## Imports and data loading\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "import gym\n",
    "\n",
    "from musicrl.midi2vec import MidiVectorMapper\n",
    "from musicrl.render import *\n",
    "from musicrl.random_generator import generate_random_midi, resemble_midi\n",
    "from musicrl.data import RandomMidiDataGenerator\n",
    "from musicrl import mel_lstm\n",
    "\n",
    "import pretty_midi\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "REAL = 1\n",
    "GEN = 0\n",
    "\n",
    "na = None # new axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T10:29:11.568313Z",
     "start_time": "2020-03-11T10:28:40.382528Z"
    }
   },
   "outputs": [],
   "source": [
    "filepaths = list(glob('maestro-v2.0.0/2008/**.midi'))\n",
    "real_midis = [pretty_midi.PrettyMIDI(i) for i in filepaths]\n",
    "mapper = MidiVectorMapper(real_midis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T10:29:11.741856Z",
     "start_time": "2020-03-11T10:29:11.570490Z"
    }
   },
   "outputs": [],
   "source": [
    "mapper = MidiVectorMapper(real_midis)\n",
    "real_seq = mapper.midi2vec(real_midis[1])\n",
    "real_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T10:29:11.832221Z",
     "start_time": "2020-03-11T10:29:11.744363Z"
    }
   },
   "outputs": [],
   "source": [
    "notes = []\n",
    "for event in real_seq:\n",
    "    if isinstance(mapper.action2note(event), pretty_midi.Note):\n",
    "        notes.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T10:29:48.495826Z",
     "start_time": "2020-03-11T10:29:48.442543Z"
    }
   },
   "outputs": [],
   "source": [
    "mapper.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T13:08:12.917206Z",
     "start_time": "2020-03-11T13:08:12.847521Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "import pretty_midi\n",
    "\n",
    "\n",
    "class MelEnvironment(gym.Env):\n",
    "    \"\"\"Environment to grain generating midi data in a self defined\n",
    "    vector space. The midi vector representation is defined via the\n",
    "    mapper object.\n",
    "    The waveform for the single instrument is then preprocessed for the\n",
    "    discriminator, and at each time step, the discriminators final prediction\n",
    "    serves as reward.\n",
    "    The preprocessed waveform, i.e. the mel spectrogram, also serves as\n",
    "    observation. The number of time frames that are used for the observation\n",
    "    are defined by the constant `self.N_TIMESTEPS`.\n",
    "    One session is understood as one song.\n",
    "    \n",
    "    Gets:\n",
    "        discriminator: keras.Model: np.array(preprocessed) -> np.array(#time_steps, 1)\n",
    "        preprocess: function: np.array(#actions): waveform -> np.array(preprocessed) : spectrogram\n",
    "        mapper: musicrl.midi2vec.MidiVectorMapper\n",
    "        N_TIMESTEPS: int: number of timesteps used to generate the observation\n",
    "        MAX_NUM_ACTIONS: int: number of actions after which to end a trajectory\n",
    "    \"\"\"\n",
    "    def __init__(self, discriminator, preprocess_wav, mapper, N_TIMESTEPS=100, MAX_NUM_ACTIONS=10000):\n",
    "        super().__init__()\n",
    "        # N_TIMESTEPS is used to define the observation:\n",
    "        # This many timeframes of the spectrogram are fed\n",
    "        # back to the generator\n",
    "        self.N_TIMESTEPS = N_TIMESTEPS\n",
    "        self.MAX_NUM_ACTIONS = MAX_NUM_ACTIONS\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = gym.spaces.Box(0, np.inf, shape=(mapper.dims,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                           shape=(self.N_TIMESTEPS, 128), dtype=np.float32)\n",
    "        self.discriminator = discriminator\n",
    "        self.preprocess_wav = preprocess_wav\n",
    "        self.mapper = mapper\n",
    "        self.fr = 44100\n",
    "        self.rewards = []\n",
    "        self.current_seq = []\n",
    "        self.current_midi = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=300)\n",
    "        self.current_midi.instruments.append(pretty_midi.Instrument(program=0))\n",
    "        self.current_observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "     \n",
    "    def _update_wav(self, action):\n",
    "        self.current_seq.append(action)\n",
    "        event = mapper.action2note(action)\n",
    "        if isinstance(event, pretty_midi.Note):\n",
    "            if len(self.current_midi.instruments[0].notes) == 1:\n",
    "                # It is the first note, so we synthesize\n",
    "                self.current_midi.instruments[0].notes.append(event)\n",
    "                self.current_midi.instruments[0].synthesize(self.fr)\n",
    "            else:\n",
    "                self.current_midi.instruments[0].append_and_synthesize(event)\n",
    "            return True\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        self._update_wav(action)\n",
    "        preprocessed = self.preprocess_wav(self.current_wav, self.fr)[na]\n",
    "        prediction = self.discriminator.predict_on_batch(preprocessed)\n",
    "        observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "        observation[-min(self.N_TIMESTEPS, len(preprocessed[0])):] = preprocessed[0, -self.N_TIMESTEPS:]\n",
    "        self.current_observation = observation\n",
    "        self.current_prediction = prediction\n",
    "        reward = prediction[0, -1, 0]\n",
    "        self.rewards.append(reward)\n",
    "        # TODO: add a end token to mapper (issue #1)\n",
    "        done = len(self.current_seq) >= self.MAX_NUM_ACTIONS\n",
    "        return observation, reward, done, None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_seq = []\n",
    "        self.current_midi = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=300)\n",
    "        self.current_midi.instruments.append(pretty_midi.Instrument(program=0))\n",
    "        self.current_observation = np.zeros((self.N_TIMESTEPS, 128))\n",
    "        self.rewards = []\n",
    "        return self.current_observation\n",
    "    \n",
    "    @property\n",
    "    def current_wav(self):\n",
    "        return self.current_midi.instruments[0].synthesized\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        plot_spectro(self.current_observation.T, \"Current observation\")\n",
    "    \n",
    "    def close (self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T13:09:34.390575Z",
     "start_time": "2020-03-11T13:08:13.422851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = load_model(\"models/mel_lstm.h5\")\n",
    "\n",
    "env = MelEnvironment(discriminator, mel_lstm.preprocess_wav, mapper, 1000)\n",
    "\n",
    "for i, action in enumerate(notes):\n",
    "    env.step(action)\n",
    "    if i % 500 == 0:\n",
    "        env.render()\n",
    "        plt.show()\n",
    "    if i > 500: break\n",
    "        \n",
    "    \n",
    "# display(Audio(env.current_wav, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T13:36:26.881575Z",
     "start_time": "2020-03-11T13:36:26.632310Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(env.rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('midi-rl': conda)",
   "language": "python",
   "name": "python37664bitmidirlcondaf7dd53044da64cd186e20414f04e4f3e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
