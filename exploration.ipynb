{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:35:01.925005Z",
     "start_time": "2020-02-27T16:35:01.909149Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning for generating music\n",
    "\n",
    "Inspired by [this](https://deepmind.com/blog/article/learning-to-generate-images) article of deepmind, we want to try to train an agent to generate music.\n",
    "When humans create music, they neither generate pure waveforms or spectrograms, instead, they choose a couple of sounds or instruments and experiment on a higher level with them. Midi data is a good abstraction for this.\n",
    "We can try to mimic this process by having an agent generate the chords, and simultanously training a discriminator. The agents reward will be the log likelihood that $D$ predicts about being music.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code structure:\n",
    "\n",
    "Discriminator:\n",
    "    - transform (midi -> mel spectro)\n",
    "    - predict\n",
    "    - train on batch\n",
    "    - Models:\n",
    "        - conv net\n",
    "        - lstm\n",
    "        - transformer\n",
    "        \n",
    "    \n",
    "\n",
    "Environment:\n",
    "    - step\n",
    "    - reset\n",
    "    - render\n",
    "    - close\n",
    "    \n",
    "Agent:\n",
    "    - next chord\n",
    "    \n",
    "    \n",
    "midi -> wav -> mel -> D\n",
    "\n",
    "Agent -> y -> midi \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Code structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.632473Z",
     "start_time": "2020-02-27T16:33:50.628427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    \"\"\"Base class for neural nets that learn to discriminate\n",
    "    generated midi from real midi data\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        \"\"\"\n",
    "        Gets:\n",
    "            sequence: List of midi events, single sample\n",
    "        Returns:\n",
    "            q: float, log likelihood of the sequence being real\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train_on_batch(self, sequences):\n",
    "        \"\"\"\n",
    "        Gets:\n",
    "            sequences: List of midi sequences\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def __transform(sequence):\n",
    "        \"\"\"\n",
    "        Gets a sequence and generates a mel spectrogram\n",
    "        which can be used for the neural net input\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.637580Z",
     "start_time": "2020-02-27T16:33:50.634181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def reset(self):\n",
    "        \"\"\"Clear the memory of the agent to start generating\n",
    "        a new song\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def next_action(self, observation):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.724326Z",
     "start_time": "2020-02-27T16:33:50.639954Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class Environment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         # Define action and observation space\n",
    "#         # They must be gym.spaces objects\n",
    "#         # Example when using discrete actions:\n",
    "#         self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "#         # Example for using image as input:\n",
    "#         self.observation_space = spaces.Box(low=0, high=255,\n",
    "#                                             shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return observation  # reward, done, info can't be included\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close (self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.727868Z",
     "start_time": "2020-02-27T16:33:50.632Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Training():\n",
    "    def __init__(self, data, env, agent):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Getting and inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.729706Z",
     "start_time": "2020-02-27T16:33:50.634Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
    "# !unzip maestro-v2.0.0-midi.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.731645Z",
     "start_time": "2020-02-27T16:33:50.636Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls maestro-v2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.733570Z",
     "start_time": "2020-02-27T16:33:50.638Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('maestro-v2.0.0/maestro-v2.0.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.736517Z",
     "start_time": "2020-02-27T16:33:50.640Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.739432Z",
     "start_time": "2020-02-27T16:33:50.642Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "# import numpy as np\n",
    "# import pretty_midi\n",
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# def piano_roll_sequences(filenames, batch_size, sequence_size, rate=100):\n",
    "#     \"\"\"Returns a dataset of piano roll sequences from the given files..\"\"\"\n",
    "\n",
    "#     def _to_piano_roll(filename, sequence_size):\n",
    "#         \"\"\"Load a file and return consecutive piano roll sequences.\"\"\"\n",
    "#         try:\n",
    "#             midi = pretty_midi.PrettyMIDI(tf.compat.as_text(filename))\n",
    "#         except Exception:\n",
    "#             print(\"Skipping corrupt MIDI file\", filename)\n",
    "#             return np.zeros([0, sequence_size, 128], dtype=np.bool)\n",
    "#         roll = np.asarray(midi.get_piano_roll(rate).transpose(), dtype=np.bool)\n",
    "#         assert roll.shape[1] == 128\n",
    "#         # Pad the roll to a multiple of sequence_size\n",
    "#         length = len(roll)\n",
    "#         remainder = length % sequence_size\n",
    "#         if remainder:\n",
    "#             new_length = length + sequence_size - remainder\n",
    "#             roll = np.resize(roll, (new_length, 128))\n",
    "#             roll[length:, :] = False\n",
    "#             length = new_length\n",
    "#         return np.reshape(roll, (length // sequence_size, sequence_size, 128))\n",
    "\n",
    "#     def _to_piano_roll_dataset(filename):\n",
    "#         \"\"\"Filename (string scalar) -> Dataset of piano roll sequences.\"\"\"\n",
    "#         sequences, = tf.py_function(_to_piano_roll,\n",
    "#                                 [filename, sequence_size],\n",
    "#                                 [tf.bool])\n",
    "#         sequences.set_shape([None, None, 128])\n",
    "#         return tf.data.Dataset.from_tensor_slices(sequences)\n",
    "\n",
    "#     batch_size = tf.cast(batch_size, tf.int64)\n",
    "#     return (tf.data.Dataset.from_tensor_slices(filenames)\n",
    "#             .interleave(_to_piano_roll_dataset,\n",
    "#                         cycle_length=batch_size * 5,\n",
    "#                         block_length=1)\n",
    "#             .repeat()\n",
    "#             .shuffle(1000)\n",
    "#             .batch(batch_size))\n",
    "\n",
    "\n",
    "# def piano_roll_to_midi(piano_roll, sample_rate):\n",
    "#     \"\"\"Convert the piano roll to a PrettyMIDI object.\n",
    "#     See: http://github.com/craffel/examples/reverse_pianoroll.py\n",
    "#     \"\"\"\n",
    "#     midi = pretty_midi.PrettyMIDI()\n",
    "#     instrument = pretty_midi.Instrument(0)\n",
    "#     midi.instruments.append(instrument)\n",
    "#     padded_roll = np.pad(piano_roll, [(1, 1), (0, 0)], mode='constant')\n",
    "#     changes = np.diff(padded_roll, axis=0)\n",
    "#     notes = np.full(piano_roll.shape[1], -1, dtype=np.int)\n",
    "#     for tick, pitch in zip(*np.where(changes)):\n",
    "#         prev = notes[pitch]\n",
    "#         if prev == -1:\n",
    "#             notes[pitch] = tick\n",
    "#             continue\n",
    "#         notes[pitch] = -1\n",
    "#         instrument.notes.append(pretty_midi.Note(\n",
    "#             velocity=100,\n",
    "#             pitch=pitch,\n",
    "#             start=prev / float(sample_rate),\n",
    "#             end=tick / float(sample_rate)))\n",
    "#     return midi\n",
    "\n",
    "\n",
    "# def write_test_note(path, duration, note):\n",
    "#     midi = pretty_midi.PrettyMIDI()\n",
    "#     instrument = pretty_midi.Instrument(0)\n",
    "#     instrument.notes.append(pretty_midi.Note(100, note, 0.0, duration))\n",
    "#     midi.instruments.append(instrument)\n",
    "#     midi.write(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty Midi\n",
    "\n",
    "Resources:\n",
    "https://nbviewer.jupyter.org/github/craffel/pretty-midi/blob/master/Tutorial.ipynb\n",
    "\n",
    "Instruments\n",
    "- How many instruments do the samples have? -> 1\n",
    "- What information is there about each instruments?\n",
    "    - Notes\n",
    "    - Control changes\n",
    "    - Pitch bends -> We dont have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:35:38.876650Z",
     "start_time": "2020-02-27T16:35:10.431522Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pretty_midi\n",
    "from glob import glob\n",
    "samples = [pretty_midi.PrettyMIDI(i) for i in glob('maestro-v2.0.0/2008/**.midi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:51:11.990178Z",
     "start_time": "2020-02-27T16:51:11.963000Z"
    }
   },
   "outputs": [],
   "source": [
    "samples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:48:13.670788Z",
     "start_time": "2020-02-27T16:48:09.192675Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([sample.estimate_tempo() for sample in samples], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "\n",
    "We can plot the notes being played per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:35:41.528498Z",
     "start_time": "2020-02-27T16:35:38.878580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
    "    # Use librosa's specshow function for displaying the piano roll\n",
    "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
    "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
    "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_piano_roll(samples[0], 56, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T23:32:30.982703Z",
     "start_time": "2020-02-27T23:32:30.943727Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def midi2wav(sample):\n",
    "    \"\"\"Generate an in-memory wav file from a PrettyMidi object\n",
    "    Gets:\n",
    "        sample: PrettMidi object\n",
    "    Returns:\n",
    "        data: np.array with 1 dimension, waveform\n",
    "        rate: int, sample rate\n",
    "    \"\"\"\n",
    "    return sample.synthesize(fs=44100), 44100\n",
    "\n",
    "\n",
    "def listen_to(sample):\n",
    "    \"\"\"Create a audio player that renders a PrettyMidi object\"\"\"\n",
    "    data, rate = midi2wav(sample)\n",
    "    display(Audio(data=data, rate=rate))\n",
    "    \n",
    "def save_as_wav(sample, filename):\n",
    "    data, rate = midi2wav(sample)\n",
    "    sf.write(filename, data, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:56:46.253847Z",
     "start_time": "2020-02-27T21:56:39.235831Z"
    }
   },
   "outputs": [],
   "source": [
    "listen_to(samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.750957Z",
     "start_time": "2020-02-27T16:33:50.653Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many instruments per sample?\n",
    "num_instruments = [len(sample.instruments) for sample in samples]\n",
    "min(num_instruments), max(num_instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:45:05.370280Z",
     "start_time": "2020-02-27T21:45:05.329396Z"
    }
   },
   "outputs": [],
   "source": [
    "[sample.instruments[0].program for sample in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:49:36.153418Z",
     "start_time": "2020-02-27T16:49:36.115024Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[sample.resolution for sample in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:52:15.202882Z",
     "start_time": "2020-02-27T16:52:15.179450Z"
    }
   },
   "outputs": [],
   "source": [
    "[sample.lyrics for sample in samples if len(sample.lyrics)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.753155Z",
     "start_time": "2020-02-27T16:33:50.656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notes of one instrument\n",
    "sorted(samples[0].instruments[0].notes, key = lambda a: a.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.755615Z",
     "start_time": "2020-02-27T16:33:50.658Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of notes per file\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([len(sample.instruments[0].notes) for sample in samples], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch bends\n",
    "Since MIDI notes are all defined to have a specific integer pitch value, in order to represent arbitrary pitch frequencies we need to use pitch bends. A PitchBend class in pretty_midi holds a time (in seconds) and a pitch offset. The pitch offset is an integer in the range [-8192, 8191], which in General MIDI spans the range from -2 to +2 semitones. As with Notes, the Instrument class has a list for PitchBend class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.757984Z",
     "start_time": "2020-02-27T16:33:50.660Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[len(sample.instruments[0].pitch_bends) for sample in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset doesn't contain it, so we will ignore this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.760550Z",
     "start_time": "2020-02-27T16:33:50.662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of control changes per file\n",
    "plt.hist([len(sample.instruments[0].control_changes) for sample in samples], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.763185Z",
     "start_time": "2020-02-27T16:33:50.666Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(samples[0].instruments[0].control_changes, key=lambda a: a.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the value mean? Should we use one hot encoding or use the numerical value?\n",
    "-> https://www.midi.org/specifications-old/item/table-3-control-change-messages-data-bytes-2\n",
    "\n",
    "Number is a categorical feature, value is a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.765689Z",
     "start_time": "2020-02-27T16:33:50.668Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_control_changes_values = [i.value for i in samples[0].instruments[0].control_changes]\n",
    "sorted(set(sample_control_changes_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T16:33:50.768275Z",
     "start_time": "2020-02-27T16:33:50.670Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = [[i.number for i in sample.instruments[0].control_changes] for sample in samples]\n",
    "sample_control_changes_number = [item for sublist in tmp for item in sublist]\n",
    "sorted(set(sample_control_changes_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midi vector Mapper class\n",
    "\n",
    "Summarizing, we got:\n",
    "- One istrument per song in our piano dataset\n",
    "- Notes that look like this Note(start=3.192708, end=3.227865, pitch=59, velocity=69)\n",
    "    - What does velocity mean? -> How fast you hit the keyboard key, i.e. volume\n",
    "- Control changes that have a categorical feature (number) and a numerical value\n",
    "    - In our dataset, only a few different control numbers occur. So for one hot encoding, we should use only as many dimensions as different numbers occur, which is why the mapper should be dataset dependent\n",
    "- No pitch bends\n",
    "- Resolution is always 384\n",
    "\n",
    "How should we encode it?\n",
    "- Sequence of events, ordered by time\n",
    "- Notes:\n",
    "    - Encode duration vs end\n",
    "    - Encode absolute start vs offset since last vs offset relative to rythm?\n",
    "    \n",
    "Should the one hot encoding -> midi category be a deterministic mapping (`[0.1, 0.5, 0.4] -> 1`) or a probabilistic mapping( $P(c | onehot) = onehot[c]$ )?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:24:46.416997Z",
     "start_time": "2020-02-27T21:24:46.360799Z"
    }
   },
   "outputs": [],
   "source": [
    "s = samples[0].instruments[0]\n",
    "s.notes[0].start, s.control_changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:57:22.306410Z",
     "start_time": "2020-02-27T21:57:15.724868Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MidiVectorMapper():\n",
    "    \"\"\"Map a PrettyMIDI object to a sequence of vectors and back.\n",
    "    For single instrument midi tracks only.\n",
    "    Gets:\n",
    "        - dataset: List of PrettyMIDI objects, to check for the categorical features, which features exist\n",
    "    \"\"\"\n",
    "    def __init__(self, samples):\n",
    "        \"\"\"\n",
    "        Dimensions:\n",
    "            0: time\n",
    "            1: is_note\n",
    "            For notes only:\n",
    "            2: pitch\n",
    "            3: velocity\n",
    "            4: duration\n",
    "            For control changes only:\n",
    "            5: value\n",
    "            6-?: one hot encoding for control number\n",
    "        \"\"\"\n",
    "        tmp = [[i.number for i in sample.instruments[0].control_changes] for sample in samples]\n",
    "        sample_control_changes_number = [item for sublist in tmp for item in sublist]\n",
    "        self.control_change_categories = sorted(set(sample_control_changes_number))\n",
    "        self.dims = 5 + len(self.control_change_categories)\n",
    "        \n",
    "    def _timeof(self, event):\n",
    "        \"\"\"Return the start time for notes or the time for control change events\n",
    "        \"\"\"\n",
    "        return event.start if isinstance(event, pretty_midi.Note) else event.time\n",
    "    \n",
    "    def midi2vec(self, sample):\n",
    "        \"\"\"Map a PrettyMIDI object to a sequence of vectors\"\"\"\n",
    "        events = sorted(\n",
    "            sample.instruments[0].notes +\n",
    "            sample.instruments[0].control_changes,\n",
    "            key=self._timeof\n",
    "        )\n",
    "        seq = np.zeros([len(events), self.dims])\n",
    "        for i, event in enumerate(events):\n",
    "            seq[i, 0] = self._timeof(event)\n",
    "            if isinstance(event, pretty_midi.Note):\n",
    "                seq[i, 1:5] = 1, event.pitch, event.velocity, event.end - event.start\n",
    "            else:\n",
    "                seq[5] = event.value\n",
    "                seq[6+self.control_change_categories.index(event.number)] = 1\n",
    "                \n",
    "        return seq\n",
    "    \n",
    "    def vec2midi(self, seq):\n",
    "        \"\"\"Map a vector to a PrettyMIDI object with a single piano\n",
    "        \"\"\"\n",
    "        song = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=300)\n",
    "        piano = pretty_midi.Instrument(program=0)\n",
    "        for event_vec in seq:\n",
    "            if event_vec[1] > 0.5:\n",
    "                piano.notes.append(\n",
    "                    pretty_midi.Note(\n",
    "                        start=event_vec[0],\n",
    "                        pitch=int(event_vec[2]),\n",
    "                        velocity=int(event_vec[3]),\n",
    "                        end=event_vec[0]+event_vec[4]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                piano.control_changes.append(\n",
    "                    pretty_midi.ControlChange(\n",
    "                        time=event_vec[0],\n",
    "                        value=int(event_vec[5]),\n",
    "                        number=self.control_change_categories[np.argmax(event_vec[6:])]\n",
    "                    )\n",
    "                )\n",
    "        song.instruments.append(piano)\n",
    "        return song\n",
    "    \n",
    "mapper = MidiVectorMapper(samples)\n",
    "seq = mapper.midi2vec(samples[0])\n",
    "reconstruction = mapper.vec2midi(seq)\n",
    "listen_to(reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:04:08.228610Z",
     "start_time": "2020-02-27T22:04:08.181445Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listen_to(samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:57:47.170419Z",
     "start_time": "2020-02-27T22:57:47.145036Z"
    }
   },
   "source": [
    "## Generating a random midi track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T23:28:32.395610Z",
     "start_time": "2020-02-27T23:28:24.206351Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand = np.zeros([8000,8])\n",
    "rand[:,0] = np.random.uniform(0, 300, size=8000) # start\n",
    "rand[:,1] = np.random.uniform(0,1,size=8000) < seq[:,1].mean() # is_note\n",
    "# split notes and control change events\n",
    "rand_notes = rand[rand[:,1]==1] # select rows there is_note is true\n",
    "rand_cc = rand[rand[:,1]==0] # select rows there is_note is false\n",
    "seq_notes = seq[seq[:,1]==1] # select rows there is_note is true\n",
    "seq_cc = seq[seq[:,1]==0] # select rows there is_note is false\n",
    "# notes: pitch, velocity, duration/end\n",
    "rand_notes[:,2:5] = np.random.multivariate_normal(seq_notes[:,2:5].mean(axis=0), np.diag(seq_notes[:,2:5].std(axis=0)), size=len(rand_notes))\n",
    "rand_notes[:,4] = np.max(rand_notes[:,4], 0)\n",
    "# events: value, one hot encodings for number\n",
    "# it doesn't really make sense to use normal distributed values for one hot\n",
    "# encoding - it should be a distribution where p(rand_one_hot.argmax()) is distributed\n",
    "# like p(rand_one_hot.argmax()), but it doesn't really matter\n",
    "rand_cc[:,5:] = np.random.multivariate_normal(seq_cc[:,5:].mean(axis=0), np.diag(seq_cc[:,5:].std(axis=0)), size=len(rand_cc))\n",
    "# copy back\n",
    "rand[rand[:,1]==1] = rand_notes\n",
    "rand[rand[:,1]==0] = rand_cc\n",
    "# columns 2-5 are 7bit ints\n",
    "rand[:,2:6] = np.clip(rand[:,2:6], 0, 127).astype(int)\n",
    "# Done!\n",
    "rand_midi = mapper.vec2midi(rand)\n",
    "listen_to(rand_midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T23:33:19.160788Z",
     "start_time": "2020-02-27T23:33:14.695723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploration.ipynb       maestro-v2.0.0-midi.zip random_sounds.wav\r\n",
      "\u001b[1m\u001b[36mmaestro-v2.0.0\u001b[m\u001b[m          maestro-v2.0.0.zip      requirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "save_as_wav(rand_midi, 'random_sounds.wav')\n",
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "373.316px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
